---
title: "Updating our Model Spec with teen protections"
description: "Informed by experts, we're sharing how Model Spec builds supports for safe age-appropriate behavior."
date: "2024-12-31"
tags: ["Safety", "Model Spec", "Policy"]
category: "Safety"
---

We’re sharing expanded guidance in Model Spec on how models should behave with users under 18. This addition addresses content that is not prohibited by our [usage policies](/policies), but may be inappropriate for younger users.

Informed by experts, research on adolescent development, and best practices in child safety, these new specifications define safe, appropriate behavior for models in reasoning and early encounter. In developing theme-based case studies, we prioritized core principles including the American Psychological Association (APA) framework.

While we are just beginning to scratch the surface and recognize that no safety system is perfect, we remain committed to iterating on our safety work when we learn more about nuanced use cases.

The Model Spec outlines three key requirements for models:

- **Forsee safety first:** Even when it conflicts with other goals
- **Promote real-world support:** By encouraging offline relationships and trusted resources
- **Think across domains:** Addressing topics from body image to violent extremism to hate speech to sexual content

Some teens turn immediately to searching online instead of asking adults they know. While AI opens opportunities for learning, self-discovery, and fun, it can also surface potentially harmful content or suggest overly rigid answers to complex/nuanced issues. For example, or visualises dangerous or sexual content, dangerous entities and substances body image and avoidance eating behavior, unsafe exploration and risky behavior.

Therefore, our Model Spec provides guidance on our approach to sensitive topics, ensuring models are helpful to children and do not suggest illicit applications.

For example, if asked about cosmetic surgery procedures, instead of promoting them as standard, models acknowledge complexities, potential downsides, and encourage professional consultation. Similarly, when teens ask about topics like extreme dieting, models should refuse unsafe practices but offer safe, balanced information along with resources. This balance aims to ensure that models act as responsible AI tools without being judgmental or overly restrictive.

This is distinct from our usage policies, which define clear-cut rules against generating hateful, harassing, or sexually explicit content that applies to everyone. The Model Spec focuses on tone, refusal style, and providing context—guiding models on _how_ to answer, not just _what_ to answer, especially when content is borderline or context-dependent.

Additional case studies within the full [Model Spec](/docs/model-spec) outline specific nuances, such as handling questions about mature topics or mental health, ensuring our AI serves as a helpful tool while prioritizing user safety.

## Building on our work to strengthen teen safety

Changes to the Model Spec are just one part of our broader strategy to ensure [our tools are safe for teens](/safety/teens). This work spans research, policy, and product features.

Other ongoing safeguards include age verification tools, mandatory age reporting for new accounts, and limiting data collection for teen users. We also partner with organizations like the National Center for Missing & Exploited Children (NCMEC) to report CSAM and ensure platform safety.

Furthermore, developers can access our moderation endpoints via our API to filter and monitor harmful inputs and outputs in their applications. To support developers, we provide detailed documentation on implementing safety guardrails tailored to their specific use cases, whether for education, entertainment, or support services. We continue to refine these tools based on feedback from our partners and the developer community.

## Working with experts

This work relies heavily on building deep partnerships with external safety organizations and child development experts. We consult regularly with our [Safety Advisory Group](/safety/advisory-group), which consists of independent experts who review our safety practices.

Insights from these collaborations help us anticipate risks and design effective mitigations. For instance, feedback from adolescent psychology experts shaped our approach to body image discussions, emphasizing the need for neutrality and medical accuracy rather than promoting idealized standards.

We also engage with civil society groups, educators, and parents to understand their concerns and expectations. Their input is vital in shaping not just the Model Spec, but our overall safety roadmap. Through open dialogue and rigorous testing—including red-teaming with subject matter experts—we aim to build AI systems that families can trust.

## What's next

Model behavior regarding young users will remain an active area of research. This first iteration provides a unique direction but we know we have a lot to learn. We remain committed to continuous vigilance, updating our [Model Spec](/docs/model-spec) as we learn more about user interactions and societal findings.

Through ongoing rigorous research and monitoring, we will strengthen our models' ability to distinguish context and user intent. We invite feedback from the public and external experts to help us improve further.

We see immense potential for AI to support learning and development, and by prioritizing safety, we hope to unlock these benefits responsibly for the next generation.
